{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82be1f75",
   "metadata": {},
   "source": [
    "# Explication de modèles NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7c85d9",
   "metadata": {},
   "source": [
    "Tous les modèles d'apprentissage automatique qui opèrent dans des dimensions supérieures à ce qui peut être directement visualisé par l'esprit humain peuvent être qualifiés de modèles boîte noire, ce qui affecte l'interprétabilité des modèles. En particulier dans le domaine du traitement du langage naturel (NLP), il est toujours le cas que les dimensions des caractéristiques sont très grandes, ce qui rend beaucoup plus compliquée l'explication de l'importance des caractéristiques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a70fda7",
   "metadata": {},
   "source": [
    "Ainsi ce notebook concistera à construire un modèle de classification de texte multi-classes, puis appliquer séparément LIME et SHAP pour l'explication et l'interprétabilité des modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0f8880",
   "metadata": {},
   "source": [
    "### 1- Importation des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c08d2a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ast import literal_eval\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d5b18a",
   "metadata": {},
   "source": [
    "### 2- Chargement et préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a71428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('EFREI - LIPSTIP - 50k elements EPO.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3df2fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee282477",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ca88e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop([\"Numero de publication\",\"date de publication\",\"IPC\",\"Numéro d'application\", \"Date d'application\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2acdaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c394c6db",
   "metadata": {},
   "source": [
    "La base de données ne contient aucune informations manquante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3efd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_duplicate = sum(df.duplicated())\n",
    "print(f\"There are {total_duplicate} duplicate data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff1e490",
   "metadata": {},
   "source": [
    "Les données sont mélangées pour s'assurer qu'elles sont réparties aléatoirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1383d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df = shuffle(df,random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4b9653",
   "metadata": {},
   "source": [
    "On utilise les 20 000 premières lignes pour l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c95d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:20000]\n",
    "print(f\"There are {len(df)} rows in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff24b8d2",
   "metadata": {},
   "source": [
    "Création d'une nouvelle colonne pour les sous-classes CPC et conversion en liste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952d3af5",
   "metadata": {},
   "source": [
    "### 3- Préparation des labels et nettoyage des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbd308d",
   "metadata": {},
   "source": [
    " Les codes CPC sont extraits et transformés en liste, puis tronqués à quatre caractères, et stockés dans une nouvelle colonne CPC_tronc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2757622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "labels = df[\"CPC\"].str.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7c5cb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = df[\"CPC\"].apply(literal_eval)\n",
    "print(labels.values[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b6d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8033e301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tronquer_codes_cpc(codes):\n",
    "    return [[code[:4] for code in liste_codes ]  for liste_codes in codes]\n",
    "            \n",
    "moi=tronquer_codes_cpc(labels)\n",
    "moi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdbfb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CPC_tronc\"]=moi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f20513",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"CPC_tronc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ce3f76",
   "metadata": {},
   "source": [
    "### Nettoyage du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bac030",
   "metadata": {},
   "source": [
    "Il définit une fonction clean_text(text) qui effectue plusieurs opérations de nettoyage sur chaque texte dans la colonne 'post' du dataframe :\n",
    "    \n",
    "Décoder le texte HTML en texte brut en utilisant BeautifulSoup.\n",
    "\n",
    "Convertir le texte en minuscules.\n",
    "\n",
    "Remplacer certains symboles spécifiques par des espaces.\n",
    "\n",
    "Supprimer certains symboles considérés comme indésirables.\n",
    "\n",
    "Supprimer les stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befd9b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").text \n",
    "    text = text.lower() \n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) \n",
    "    text = BAD_SYMBOLS_RE.sub('', text) \n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580fafa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['description'] = df['description'].apply(clean_text)\n",
    "#print(df['description'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18a08ea",
   "metadata": {},
   "source": [
    "### 4- Division du jeu de données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5ee2c5",
   "metadata": {},
   "source": [
    "Cette partie conciste à Nous allons maintenant diviser le jeu de données en ensembles d'entraînement, de validation et de test en utilisant 10 % des données pour le test et 50 % des données de test pour l'ensemble de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d93bf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=10000) \n",
    "tokenizer.fit_on_texts(df['description'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['description'])\n",
    "sequences = pad_sequences(sequences, maxlen=200)  \n",
    "\n",
    "x_train = sequences[:int(0.8 * len(sequences))]\n",
    "x_val = sequences[int(0.8 * len(sequences)):]\n",
    "\n",
    "y_train = to_categorical(df['CPC_tronc'].values[:int(0.8 * len(df))], num_classes=len(set(df['CPC_tronc'].values)))\n",
    "y_val = to_categorical(df['CPC_tronc'].values[int(0.8 * len(df)):], num_classes=len(set(df['CPC_tronc'].values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e782d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64),\n",
    "    tf.keras.layers.LSTM(128, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(len(set(df['CPC_tronc'].values)), activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d2fd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95c6167",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_test = sequences[int(0.8 * len(sequences)):]\n",
    "y_test = to_categorical(df['CPC_tronc'].values[int(0.8 * len(df)) :], num_classes=len(set(df['CPC_tronc'].values)))\n",
    "\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print('Perte:', loss)\n",
    "print('Précision:', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a52ec1b",
   "metadata": {},
   "source": [
    "### 5- Préparation des labels pour le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f4a6c8",
   "metadata": {},
   "source": [
    "Dans cette étape, nous allons préparer les étiquettes pour l'entraînement en utilisant la couche StringLookup de TensorFlow, qui convertit les labels sous forme de texte en encodages numériques (multi-hot) que le modèle peut utiliser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa46b858",
   "metadata": {},
   "source": [
    "Multi-hot : Chaque label est représenté par un vecteur binaire où chaque dimension correspond à un label possible. Si un document est associé à plusieurs labels, ces labels sont tous activés dans le vecteur multi-hot.\n",
    "Cela permet au modèle de traiter des problèmes de classification multi-label, où un échantillon peut appartenir à plusieurs catégories simultanément.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5b3772",
   "metadata": {},
   "source": [
    "On extrait le vocabulaire (les labels uniques) de la couche StringLookup. Ensuite, on nettoie les entrées du vocabulaire en supprimant les espaces et les caractères superflus pour obtenir une liste propre de labels uniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccae7ca5",
   "metadata": {},
   "source": [
    "La fonction invert_multi_hot permet de convertir un vecteur multi-hot encodé en une liste de labels lisibles, facilitant ainsi l'interprétation des résultats du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b06cfc3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Subclass_labels = tf.ragged.constant(train_df[\"CPC_tronc\"].values)\n",
    "lookup = tf.keras.layers.StringLookup(output_mode=\"multi_hot\")\n",
    "lookup.adapt(Subclass_labels)\n",
    "vocab = lookup.get_vocabulary()\n",
    "cleaned_vocab = [entry.strip().strip(\"[]'\") for entry in vocab if entry.strip()]\n",
    "\n",
    "\n",
    "def invert_multi_hot(encoded_labels):\n",
    "    \"\"\"Reverse a single multi-hot encoded label to a tuple of vocab Subclass_labels.\"\"\"\n",
    "    hot_indices = np.argwhere(encoded_labels == 1.0)[..., 0]\n",
    "    return np.take(cleaned_vocab, hot_indices)\n",
    "\n",
    "\n",
    "print(\"Vocabulary:\\n\")\n",
    "#print(cleaned_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774d8ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cleaned_vocab)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d25d8d",
   "metadata": {},
   "source": [
    "Un exemple pour montrer comment une étiquette de sous-classe est extraite d'un DataFrame, puis transformée en une représentation binarisée utilisant la couche lookup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a273dee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_label = train_df[\"CPC_tronc\"].iloc[0]\n",
    "print(f\"Original label: {sample_label}\")\n",
    "\n",
    "label_binarized = lookup([sample_label])\n",
    "print(f\"Label-binarized representation: {label_binarized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9798bc",
   "metadata": {},
   "source": [
    "### 6- Préparation des jeux de données pour TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0dbdf4",
   "metadata": {},
   "source": [
    "Nous devons préparer les textes en assurant qu'ils ont une longueur unifiée pour pouvoir les passer dans le modèle. Pour cela, nous allons:Définir la longueur maximale de séquence et le batch size, puis la fonction unify_text_length sépare le texte en mots, calcule sa longueur et la quantité de padding nécessaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ba02c2",
   "metadata": {},
   "source": [
    "Notre max_seqlen fixé à 400, alors tous les textes plus longs seront tronqués pour ne garder que les 400 premiers mots, et les textes plus courts seront remplis avec des tokens spéciaux (comme <pad>) pour atteindre cette longueur maximale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724e5bf4",
   "metadata": {},
   "source": [
    "Uniformiser la longueur des séquences de texte pour qu'elles aient toutes la même longueur, facilitant ainsi leur traitement par le modèle de machine learning. En effet, pour qu'un batch de données puisse être traité efficacement par un modèle, chaque exemple dans le batch doit avoir la même forme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c613f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seqlen = 400\n",
    "batch_size = 32\n",
    "padding_token = \"<pad>\"\n",
    "auto = tf.data.AUTOTUNE\n",
    "\n",
    "def unify_text_length(text, label):\n",
    "    word_splits = tf.strings.split(text, sep=\" \")\n",
    "    sequence_length = tf.shape(word_splits)[0]\n",
    "\n",
    "    padding_amount = max_seqlen - sequence_length\n",
    "\n",
    "    if padding_amount > 0:\n",
    "        unified_text = tf.pad([text], [[0, padding_amount]], constant_values=padding_token)\n",
    "        unified_text = tf.strings.reduce_join(unified_text, separator=\"\")\n",
    "    else:\n",
    "        unified_text = tf.strings.reduce_join(word_splits[:max_seqlen], separator=\" \")\n",
    "\n",
    "    return tf.expand_dims(unified_text, -1), label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8720d7",
   "metadata": {},
   "source": [
    "La fonction make_dataset crée un dataset TensorFlow prêt pour l'entraînement, la validation et le test en transformant les textes et les labels en un format que le modèle peut traiter efficacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505c73b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(dataframe, is_train=True):\n",
    "    labels = tf.ragged.constant(dataframe[\"CPC_tronc\"].values)\n",
    "    label_binarized = lookup(labels).numpy()\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (dataframe[\"description\"].values, label_binarized)\n",
    "    )\n",
    "    dataset = dataset.shuffle(batch_size * 10) if is_train else dataset\n",
    "    dataset = dataset.map(unify_text_length, num_parallel_calls=auto).cache()\n",
    "    return dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa691245",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = make_dataset(train_df, is_train=True)\n",
    "validation_dataset = make_dataset(val_df, is_train=False)\n",
    "test_dataset = make_dataset(test_df, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6989a48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5bdb6f",
   "metadata": {},
   "source": [
    "Chaque élément de train_dataset est un tuple contenant deux éléments : un tenseur représentant un texte (tf.Tensor de type string).\n",
    "et un tenseur représentant les labels encodés en multi-hot (tf.Tensor de type int64)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55990a9b",
   "metadata": {},
   "source": [
    "#### Quelques exemples de textes et leurs labels correspondants après le traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364f18ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch, label_batch = next(iter(train_dataset))\n",
    "\n",
    "for i, text in enumerate(text_batch[:3]):\n",
    "    label = label_batch[i].numpy()[None, ...]\n",
    "    print(f\"description: {text[0]}\")\n",
    "    print(f\"Label(s): {invert_multi_hot(label[0])}\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f75daeb",
   "metadata": {},
   "source": [
    "### 7- Vectorisation des textes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133e5607",
   "metadata": {},
   "source": [
    "Cette partie du code calcule la taille maximale du vocabulaire basé sur la longueur des textes dans le dataframe d'entraînement (train_df) et ressort la taille du plus long texte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c28a7eb",
   "metadata": {},
   "source": [
    "Notre vocabulary_size est 10571, cela signifie que le modèle ne prendra en compte que les 10571 mots les plus fréquents (ou n-grammes) dans l'ensemble de données. Les mots moins fréquents peuvent être ignorés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935f21c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df[\"total_words\"] = train_df[\"description\"].str.split().str.len()\n",
    "vocabulary_size = train_df[\"total_words\"].max()\n",
    "print(f\"Vocabulary size: {vocabulary_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1e771e",
   "metadata": {},
   "source": [
    "Une couche TextVectorization de Keras est utilisée pour convertir les textes en vecteurs de TF-IDF. map : Applique une fonction à chaque élément du dataset. Ici, la fonction lambda applique text_vectorizer pour transformer les textes en vecteurs de TF-IDF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acea9e18",
   "metadata": {},
   "source": [
    " Utiliser des n-grammes et TF-IDF permet de capturer des informations importantes sur la fréquence des mots et leur importance relative dans le corpus, ce qui peut améliorer la performance du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317232ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer = layers.TextVectorization(\n",
    "    max_tokens=vocabulary_size, ngrams=2, output_mode=\"tf_idf\"\n",
    ")\n",
    "\n",
    "with tf.device(\"/CPU:0\"):\n",
    "    text_vectorizer.adapt(train_dataset.map(lambda text, label: text))\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto\n",
    ").prefetch(auto)\n",
    "validation_dataset = validation_dataset.map(\n",
    "    lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto\n",
    ").prefetch(auto)\n",
    "test_dataset = test_dataset.map(\n",
    "    lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto\n",
    ").prefetch(auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ccae67",
   "metadata": {},
   "source": [
    "### 8- Définition et compilation du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a2be69",
   "metadata": {},
   "source": [
    "Le modèle de deep learning utilisé dans votre programme est un réseau de neurones multicouche (MLP pour Multi-Layer Perceptron). Lorsque des données sont introduites dans le modèle, elles passent à travers chaque couche.\n",
    "Chaque couche applique une transformation linéaire suivie d'une fonction d'activation non linéaire (dans ce cas, \"relu\" pour les couches cachées et \"sigmoid\" pour la couche de sortie)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205f1d0d",
   "metadata": {},
   "source": [
    "make_model() : Cette fonction crée et retourne un modèle séquentiel (Sequential) de Keras, qui est une séquence linéaire de couches.Elles ajoute 3 couches denses.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd319c7e",
   "metadata": {},
   "source": [
    "Première couche dense : Composée de 512 neurones avec une fonction d'activation ReLU. Cela permet au modèle d'apprendre des représentations non linéaires à partir des données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eaac97",
   "metadata": {},
   "source": [
    "Deuxième couche dense : Composée de 256 neurones avec une fonction d'activation ReLU. Cette couche réduit progressivement la dimensionnalité des représentations apprises par le modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2369e488",
   "metadata": {},
   "source": [
    "Troisième couche dense : C'est la couche de sortie du modèle. Le nombre de neurones correspond à la taille du vocabulaire déterminé par lookup.vocabulary_size(), et l'activation \"sigmoid\" est utilisée pour une classification multi-label où chaque neurone de sortie prédit une probabilité indépendante d'appartenance à une classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0424eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    shallow_mlp_model = keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(512, activation=\"relu\"),\n",
    "            layers.Dense(256, activation=\"relu\"),\n",
    "            layers.Dense(lookup.vocabulary_size(), activation=\"sigmoid\"),\n",
    "        ]  \n",
    "    )\n",
    "    return shallow_mlp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c942ba",
   "metadata": {},
   "source": [
    "recall_m et precision_m: Calculent respectvement le rappel et la précision en utilisant la formule standard basée sur les vrais positifs et les faux négatifs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cb2a4e",
   "metadata": {},
   "source": [
    "f1_m : Calcule la F1-score en utilisant la formule harmonique de la précision et du rappel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb96e344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7668912",
   "metadata": {},
   "source": [
    " La fonction make_model() retourne un modèle séquentiel (Sequential) de Keras, qui est une séquence linéaire de couches. Ce modèle est assigné à la variable shallow_mlp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3697fc7a",
   "metadata": {},
   "source": [
    "loss=\"binary_crossentropy\" : Une fois que les données ont été propagées à travers le modèle et que les prédictions sont générées, la fonction de perte \"binary_crossentropy\" mesure à quel point les prédictions du modèle sont éloignées des étiquettes réelles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ef3479",
   "metadata": {},
   "source": [
    "optimizer=\"adam\" : C'est l'optimiseur utilisé pour minimiser la fonction de perte. \"adam\" est un optimiseur populaire en apprentissage profond en raison de son efficacité et de sa capacité à gérer les grands ensembles de données et les réseaux de neurones profonds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7f160e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shallow_mlp_model = make_model()\n",
    "shallow_mlp_model.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"categorical_accuracy\",f1_m,precision_m, recall_m]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf728f2c",
   "metadata": {},
   "source": [
    "### 9- Entraînement du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97ba84b",
   "metadata": {},
   "source": [
    "epochs = 2 signifie que le modèle sera entraîné sur l'ensemble des données d'entraînement deux fois. Plus le nombre d'époques est élevé, plus le modèle a d'opportunités d'apprendre à partir des données, mais il doit être choisi judicieusement pour éviter le surapprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bff8081",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs=2\n",
    "history = shallow_mlp_model.fit(\n",
    "    train_dataset, validation_data=validation_dataset, epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428e2273",
   "metadata": {},
   "source": [
    "Le nombre 563 dans indique le nombre total de batches que le modèle a traités pendant une époque complète d'entraînement, sachant qu'on a un dataset d'entrainement de 9000 textes et que chaque batch contient 32 échantillons ( batc_size = 32) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46afdf53",
   "metadata": {},
   "source": [
    "### 10-Visualisation des performances du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a988fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bc653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = len(history.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e44aeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.subplot(1, 2, 1) \n",
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epochs\", fontsize=16)\n",
    "plt.ylabel(\"loss\", fontsize=16)\n",
    "plt.xticks(np.arange(a), np.arange(1, a+1), fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title(\"Train and Validation loss Over Epochs\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# plt.show()\n",
    "plt.subplot(1, 2, 2) \n",
    "plt.plot(history.history[\"f1_m\"], label=\"f1_m\")\n",
    "plt.plot(history.history[\"val_f1_m\"], label=\"val_f1_m\")\n",
    "plt.xlabel(\"Epochs\",fontsize=16)\n",
    "plt.ylabel(\"accuracy\",fontsize=16)\n",
    "plt.xticks(np.arange(a), np.arange(1, a+1), fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title(\"Train and Validation f1 score Over Epochs\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9969fc7e",
   "metadata": {},
   "source": [
    "#### Évaluation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d99869",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, categorical_acc, f1_m, precision_m, recall_m = shallow_mlp_model.evaluate(test_dataset)\n",
    "print(f\"accuracy on the test set: {round(categorical_acc * 100, 2)}%.\")\n",
    "print(f\"f1 on the test set: {round(f1_m * 100, 2)}%.\")\n",
    "print(f\"precision on the test set: {round(precision_m * 100, 2)}%.\")\n",
    "print(f\"recall on the test set: {round(recall_m * 100, 2)}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b465db0a",
   "metadata": {},
   "source": [
    "#### Inférence avec le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479c7b5c",
   "metadata": {},
   "source": [
    "Un petit ensemble de données est utilisé pour montrer comment le modèle fait des prédictions. Pour chaque texte, les labels réels et les labels prédits sont affichés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff794c71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_for_inference = keras.Sequential([text_vectorizer, shallow_mlp_model])\n",
    "\n",
    "inference_dataset = make_dataset(test_df.sample(100), is_train=False)\n",
    "text_batch, label_batch = next(iter(inference_dataset))\n",
    "predicted_probabilities = model_for_inference.predict(text_batch)\n",
    "\n",
    "for i, text in enumerate(text_batch[:5]):\n",
    "    label = label_batch[i].numpy()[None, ...]\n",
    "    print(f\"Abstract: {text[0]}\")\n",
    "    print(f\"Label(s): {invert_multi_hot(label[0])}\")\n",
    "    predicted_proba = [proba for proba in predicted_probabilities[i]]\n",
    "    top_3_labels = [\n",
    "        x\n",
    "        for _, x in sorted(\n",
    "            zip(predicted_probabilities[i], lookup.get_vocabulary()),\n",
    "            key=lambda pair: pair[0],\n",
    "            reverse=True,\n",
    "        )\n",
    "    ][:3]\n",
    "    print(f\"Predicted Label(s): ({', '.join([label for label in top_3_labels])})\")\n",
    "    print(\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
